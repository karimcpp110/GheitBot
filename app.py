import streamlit as st
import requests
import pandas as pd
import folium
from streamlit_folium import st_folium
import feedparser
from difflib import get_close_matches
from bs4 import BeautifulSoup
import datetime
import re
import plotly.express as px
from streamlit_autorefresh import st_autorefresh

# -------------------------------------------------------
# ðŸ”‘ OpenWeather API Key
# -------------------------------------------------------
OPENWEATHER_API_KEY = "" # add yours

# -------------------------------------------------------
# ðŸ“‚ Load Egypt places
# -------------------------------------------------------
try:
    places_df = pd.read_csv("egypt_places.csv")
except Exception as e:
    st.error(f"[ERROR] Failed to load egypt_places.csv: {e}")
    places_df = pd.DataFrame()

# -------------------------------------------------------
# âš™ï¸ Streamlit UI Setup
# -------------------------------------------------------
st.set_page_config(page_title="ØºÙŠØ· Ø¨ÙˆØª - Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„ÙÙ„Ø§Ø­", layout="wide")
st.title("ðŸŒ¾ ØºÙŠØ· Ø¨ÙˆØª - Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„ÙÙ„Ø§Ø­ Ø§Ù„Ù…ØµØ±ÙŠ")

tab1, tab2, tab3 = st.tabs(["ðŸ’¬ Ø§Ù„Ø´Ø§Øª", "ðŸ—º Ø§Ù„Ø®Ø±ÙŠØ·Ø©", "ðŸ“° Ø§Ù„Ø£Ø®Ø¨Ø§Ø± ÙˆØ§Ù„Ø£Ø³Ø¹Ø§Ø±"])

# -------------------------------------------------------
# ðŸ’¬ Chat
# -------------------------------------------------------
with tab1:
    st.subheader("Ø§Ø³Ø£Ù„ Ø³Ø¤Ø§Ù„Ùƒ Ø¨Ø§Ù„Ø¹Ø§Ù…ÙŠØ© ðŸ‘¨â€ðŸŒ¾")
    user_q = st.text_input("ðŸ‘¨â€ðŸŒ¾ Ø§ÙƒØªØ¨ Ù‡Ù†Ø§:")

    if user_q and not places_df.empty:
        all_places = pd.concat([
            places_df["name"].dropna(),
            places_df["name_en"].dropna()
        ]).unique()

        closest = get_close_matches(user_q, all_places, n=1, cutoff=0.5)
        matched = places_df[
            (places_df["name"] == closest[0]) | (places_df["name_en"] == closest[0])
        ] if closest else pd.DataFrame()

        if not matched.empty:
            row = matched.iloc[0]
            lat, lon, city_ar = row["lat"], row["lon"], row["name"]

            if OPENWEATHER_API_KEY:
                url = f"http://api.openweathermap.org/data/2.5/weather?lat={lat}&lon={lon}&appid={OPENWEATHER_API_KEY}&units=metric&lang=ar"
                try:
                    data = requests.get(url, timeout=10).json()
                except:
                    data = {}

                if "main" in data:
                    temp = data["main"]["temp"]
                    wind = data["wind"]["speed"]
                    desc = data["weather"][0]["description"]

                    response = f"ÙÙŠ {city_ar}: Ø§Ù„Ø­Ø±Ø§Ø±Ø© {temp}Â°ØŒ {desc}ØŒ ÙˆØ§Ù„Ø±ÙŠØ§Ø­ {wind} ÙƒÙ…/Ø³ ðŸŒ¬.\n"
                    if "Ø·Ù…Ø§Ø·Ù…" in user_q:
                        response += "ðŸ… Ø§Ù„Ø·Ù…Ø§Ø·Ù… Ø¨ØªØ­Ø¨ Ø§Ù„Ø¬Ùˆ Ø§Ù„Ù…Ø¹ØªØ¯Ù„ØŒ Ù„Ùˆ Ø§Ù„Ø¬Ùˆ Ø­Ø± Ø§Ø³ØªÙ†Ù‰ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ†."
                    elif "Ù‚Ù…Ø­" in user_q:
                        response += "ðŸŒ¾ Ø§Ù„Ù‚Ù…Ø­ Ø¨ÙŠØªØ²Ø±Ø¹ Ù…Ù† Ù†Øµ Ø£ÙƒØªÙˆØ¨Ø± Ù„Ø­Ø¯ Ù†Øµ Ù†ÙˆÙÙ…Ø¨Ø±."
                    else:
                        if temp > 30:
                            response += "ðŸ”¥ Ø§Ù„Ø¬Ùˆ Ø­Ø±ØŒ Ù‚Ù„Ù„ Ø§Ù„Ø±ÙŠÙ‘."
                        elif temp < 15:
                            response += "â„ Ø§Ù„Ø¬Ùˆ Ø¨Ø±Ø¯ØŒ ØºØ·ÙŠ Ø§Ù„Ø²Ø±Ø¹."
                        else:
                            response += "ðŸŒ¿ Ø§Ù„Ø¬Ùˆ Ù…Ø¹ØªØ¯Ù„ØŒ ØªÙ…Ø§Ù… Ù„Ù„Ø²Ø±Ø§Ø¹Ø©."
                    st.success(response)
                else:
                    st.error("Ù…Ù‚Ø¯Ø±ØªØ´ Ø£Ø¬ÙŠØ¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø·Ù‚Ø³ Ø¯Ù„ÙˆÙ‚ØªÙŠ.")
            else:
                st.warning("âš  Ù…Ù† ÙØ¶Ù„Ùƒ Ø¶ÙŠÙ OpenWeather API Key.")
        else:
            st.error("Ù…Ø¹Ù„Ø´ØŒ Ù…Ù„Ù‚ØªØ´ Ø§Ù„Ù…ÙƒØ§Ù† Ø¯Ù‡. Ø¬Ø±Ø¨ ØªÙƒØªØ¨ Ø§Ø³Ù… ØªØ§Ù†ÙŠ.")

# -------------------------------------------------------
# ðŸ—º Map + Crops Advice
# -------------------------------------------------------
with tab2:
    if not places_df.empty:
        st.subheader("Ø§Ø®ØªØ§Ø± Ù…ÙƒØ§Ù† Ù…Ù† Ù…ØµØ±:")
        city_display = st.selectbox("Ø§Ø®ØªØ± Ø§Ù„Ù…ÙƒØ§Ù†:", places_df["name"].dropna().unique())

        row = places_df[places_df["name"] == city_display].iloc[0]
        lat, lon, city_ar = row["lat"], row["lon"], row["name"]

        m = folium.Map(location=[lat, lon], zoom_start=7)
        folium.Marker([lat, lon], tooltip=city_ar).add_to(m)
        st_folium(m, width=700, height=500)

        try:
            crops_df = pd.read_csv("crops_egypt.csv")
        except:
            crops_df = pd.DataFrame()

        if OPENWEATHER_API_KEY:
            url = f"http://api.openweathermap.org/data/2.5/weather?lat={lat}&lon={lon}&appid={OPENWEATHER_API_KEY}&units=metric&lang=ar"
            try:
                response = requests.get(url, timeout=10).json()
            except:
                response = {}

            if "main" in response and not crops_df.empty:
                temp = response["main"]["temp"]
                wind = response["wind"]["speed"]
                humidity = response["main"]["humidity"]
                desc = response["weather"][0]["description"]

                st.success(f"ðŸ“ {city_ar}: Ø§Ù„Ø­Ø±Ø§Ø±Ø© {temp}Â°ØŒ Ø§Ù„Ø±Ø·ÙˆØ¨Ø© {humidity}%ØŒ {desc}ØŒ Ø§Ù„Ø±ÙŠØ§Ø­ {wind} ÙƒÙ…/Ø³ ðŸŒ¬")

                chosen_crop = st.selectbox("Ø§Ø®ØªØ± Ø§Ù„Ù…Ø­ØµÙˆÙ„:", crops_df["Ø§Ù„Ù…Ø­ØµÙˆÙ„"].unique())
                crop_row = crops_df[crops_df["Ø§Ù„Ù…Ø­ØµÙˆÙ„"] == chosen_crop].iloc[0]

                st.markdown(f"### ðŸŒ± {chosen_crop}")
                st.write(f"ðŸ“… Ù…ÙˆØ³Ù… Ø§Ù„Ø²Ø±Ø§Ø¹Ø©: {crop_row['Ù…ÙˆØ³Ù…_Ø§Ù„Ø²Ø±Ø§Ø¹Ø©']}")
                st.write(f"ðŸŒ Ù†ÙˆØ¹ Ø§Ù„ØªØ±Ø¨Ø©: {crop_row['Ù†ÙˆØ¹_Ø§Ù„ØªØ±Ø¨Ø©']}")
                st.write(f"ðŸ’§ Ø§Ø­ØªÙŠØ§Ø¬Ø§Øª Ø§Ù„Ø±ÙŠ: {crop_row['Ø§Ø­ØªÙŠØ§Ø¬Ø§Øª_Ø§Ù„Ø±ÙŠ']}")
                st.write(f"ðŸ§ª Ø§Ù„ØªØ³Ù…ÙŠØ¯: {crop_row['Ø§Ù„ØªØ³Ù…ÙŠØ¯']}")
                st.write(f"â˜€ Ø§Ù„Ù…Ù†Ø§Ø® Ø§Ù„Ù…Ù†Ø§Ø³Ø¨: {crop_row['Ø§Ù„Ù…Ù†Ø§Ø®_Ø§Ù„Ù…Ù†Ø§Ø³Ø¨']}")
                st.write(f"ðŸ“ Ù…Ù„Ø§Ø­Ø¸Ø§Øª: {crop_row['Ù…Ù„Ø§Ø­Ø¸Ø§Øª']}")

                if "Ø­Ø§Ø±" in crop_row["Ø§Ù„Ù…Ù†Ø§Ø®_Ø§Ù„Ù…Ù†Ø§Ø³Ø¨"] and temp >= 28:
                    advice = f"ðŸ‘Œ Ø§Ù„Ø¬Ùˆ Ù…Ù†Ø§Ø³Ø¨ Ù„Ø²Ø±Ø§Ø¹Ø© {chosen_crop} (Ø­Ø§Ø±)."
                elif "Ø¨Ø§Ø±Ø¯" in crop_row["Ø§Ù„Ù…Ù†Ø§Ø®_Ø§Ù„Ù…Ù†Ø§Ø³Ø¨"] and temp <= 18:
                    advice = f"ðŸ‘Œ Ø§Ù„Ø¬Ùˆ Ù…Ù†Ø§Ø³Ø¨ Ù„Ø²Ø±Ø§Ø¹Ø© {chosen_crop} (Ø¨Ø§Ø±Ø¯)."
                elif "Ù…Ø¹ØªØ¯Ù„" in crop_row["Ø§Ù„Ù…Ù†Ø§Ø®_Ø§Ù„Ù…Ù†Ø§Ø³Ø¨"] and 18 < temp < 28:
                    advice = f"ðŸ‘Œ Ø§Ù„Ø¬Ùˆ Ù…Ù†Ø§Ø³Ø¨ Ù„Ø²Ø±Ø§Ø¹Ø© {chosen_crop} (Ù…Ø¹ØªØ¯Ù„)."
                else:
                    advice = f"âš  Ø§Ù„Ø¸Ø±ÙˆÙ Ø§Ù„Ø­Ø§Ù„ÙŠØ© Ù…Ø´ Ù…Ø«Ø§Ù„ÙŠØ© Ù„Ù€ {chosen_crop}."
                st.info(advice)

# -------------------------------------------------------
# ðŸ“° News + Prices
# -------------------------------------------------------
def extract_prices_from_articles(entries, crop_list):
    price_data = []
    for item in entries:
        try:
            r = requests.get(item["link"], timeout=10)
            soup = BeautifulSoup(r.content, "lxml")
            text = soup.get_text(" ", strip=True)

            for crop in crop_list:
                match = re.search(rf"{crop}[^0-9]{{0,20}}(\d+)(?:\s*[-â€“Ø¥Ù„Ù‰]\s*(\d+))?\s*(?:Ø¬Ù†ÙŠÙ‡|Ø¬|EGP)", text)
                if match:
                    if match.group(2):
                        price = f"{match.group(1)} - {match.group(2)}"
                    else:
                        price = match.group(1)
                    price_data.append({"Ø§Ù„Ù…Ø­ØµÙˆÙ„": crop, "Ø§Ù„Ø³Ø¹Ø± (Ø¬Ù†ÙŠÙ‡)": price, "Ø§Ù„Ù…ØµØ¯Ø±": item["source"]})
        except:
            continue
    return pd.DataFrame(price_data)

with tab3:
    st.subheader("ðŸ“° Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø²Ø±Ø§Ø¹ÙŠØ© + ðŸ’° Ø£Ø³Ø¹Ø§Ø± Ø§Ù„Ù…Ø­Ø§ØµÙŠÙ„ (ØªØªØ­Ø¯Ø« ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚ØªÙŠÙ†)")
    st_autorefresh(interval=2 * 60 * 1000, key="refresh_news")

    feeds = {
        "Ø£Ø®Ø¨Ø§Ø± Ø²Ø±Ø§Ø¹ÙŠØ© (Agr-Egypt)": "https://www.agr-egypt.com/feed/",
        "Ø²Ø±Ø§Ø¹Ø© Ù…ØµØ± (Google News)": "https://news.google.com/rss/search?q=Ø²Ø±Ø§Ø¹Ø©+Ù…ØµØ±&hl=ar&gl=EG&ceid=EG:ar",
        "Ø·Ù‚Ø³ (Ø¨ÙˆØ§Ø¨Ø© Ø§Ù„Ø£Ù‡Ø±Ø§Ù…)": "https://gate.ahram.org.eg/rss/97.aspx"
    }

    entries = []
    for src_name, url in feeds.items():
        try:
            for entry in feedparser.parse(url).entries[:5]:
                entries.append({
                    "source": src_name,
                    "title": entry.title,
                    "link": entry.link,
                    "published": getattr(entry, "published", "â€”")
                })
        except:
            continue

    if entries:
        st.markdown("### ðŸ“° Ø£Ø­Ø¯Ø« Ø§Ù„Ø£Ø®Ø¨Ø§Ø±")
        for it in entries[:10]:
            with st.expander(f"{it['title']}"):
                st.write(f"ðŸ“Œ {it['source']} | ðŸ—“ {it['published']}")
                st.markdown(f"[ðŸ”— Ø§Ù‚Ø±Ø£ Ø§Ù„Ø®Ø¨Ø±]({it['link']})")
    else:
        st.warning("âš  Ù…Ø´ Ù„Ø§Ù‚ÙŠØª Ø£Ø®Ø¨Ø§Ø± Ø¯Ù„ÙˆÙ‚ØªÙŠ.")

    try:
        crops_df = pd.read_csv("crops_egypt.csv")
        crop_list = crops_df["Ø§Ù„Ù…Ø­ØµÙˆÙ„"].dropna().unique()
    except:
        crop_list = []

    st.markdown("### ðŸ“Š Ø£Ø³Ø¹Ø§Ø± Ø§Ù„Ù…Ø­Ø§ØµÙŠÙ„")
    df_prices = extract_prices_from_articles(entries, crop_list)
    if not df_prices.empty:
        st.dataframe(df_prices)
        fig = px.bar(df_prices, x="Ø§Ù„Ù…Ø­ØµÙˆÙ„", y="Ø§Ù„Ø³Ø¹Ø± (Ø¬Ù†ÙŠÙ‡)", color="Ø§Ù„Ù…ØµØ¯Ø±", text="Ø§Ù„Ø³Ø¹Ø± (Ø¬Ù†ÙŠÙ‡)")
        fig.update_traces(textposition="outside")
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("â„¹ Ù„Ø³Ù‡ Ù…Ù„Ø§Ù‚ÙŠØªØ´ Ø£Ø³Ø¹Ø§Ø± Ù„Ù„Ù…Ø­Ø§ØµÙŠÙ„ ÙÙŠ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±.")

    st.caption(f"â± Ø¢Ø®Ø± ØªØ­Ø¯ÙŠØ«: {datetime.datetime.now().strftime('%d-%m-%Y %H:%M:%S')}")
