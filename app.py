import streamlit as st
import requests
import pandas as pd
import folium
from streamlit_folium import st_folium
import feedparser
from difflib import get_close_matches
from bs4 import BeautifulSoup
import datetime
import re

# Load Egypt places
try:
    places_df = pd.read_csv("egypt_places.csv")
    print("[INFO] egypt_places.csv loaded successfully.")
except Exception as e:
    print(f"[ERROR] Failed to load egypt_places.csv: {e}")

# Weather API Key
OPENWEATHER_API_KEY = "" #add yours ðŸ¤¨
 

# --- UI Setup ---
st.set_page_config(page_title="ØºÙŠØ· Ø¨ÙˆØª - Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„ÙÙ„Ø§Ø­", layout="wide")
st.title("ðŸŒ¾ ØºÙŠØ· Ø¨ÙˆØª - Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„ÙÙ„Ø§Ø­ Ø§Ù„Ù…ØµØ±ÙŠ")

tab1, tab2, tab3 = st.tabs(["ðŸ’¬ Ø§Ù„Ø´Ø§Øª", "ðŸ—º Ø§Ù„Ø®Ø±ÙŠØ·Ø©", "ðŸ“° Ø§Ù„Ø£Ø®Ø¨Ø§Ø± ÙˆØ§Ù„Ø£Ø³Ø¹Ø§Ø±"])

# -------------------------------------------------------
# ðŸ’¬ Chat
# -------------------------------------------------------
with tab1:
    st.subheader("Ø§Ø³Ø£Ù„ Ø³Ø¤Ø§Ù„Ùƒ Ø¨Ø§Ù„Ø¹Ø§Ù…ÙŠØ© ðŸ‘¨â€ðŸŒ¾")
    user_q = st.text_input("ðŸ‘¨â€ðŸŒ¾ Ø§ÙƒØªØ¨ Ù‡Ù†Ø§:")

    if user_q:
        all_places = pd.concat([
            places_df["name"].dropna(),
            places_df["name_en"].dropna()
        ]).unique()

        closest = get_close_matches(user_q, all_places, n=1, cutoff=0.5)

        if closest:
            place_name = closest[0]
            matched = places_df[(places_df["name"] == place_name) | (places_df["name_en"] == place_name)]
        else:
            matched = pd.DataFrame()

        if not matched.empty:
            row = matched.iloc[0]
            lat, lon, city_ar = row["lat"], row["lon"], row["name"]

            if OPENWEATHER_API_KEY:
                url = f"http://api.openweathermap.org/data/2.5/weather?lat={lat}&lon={lon}&appid={OPENWEATHER_API_KEY}&units=metric&lang=ar"
                try:
                    data = requests.get(url, timeout=10).json()
                    print(f"[INFO] Weather API response for {city_ar}: OK")
                except Exception as e:
                    print(f"[ERROR] Weather API request failed: {e}")
                    data = {}

                if "main" in data:
                    temp = data["main"]["temp"]
                    wind = data["wind"]["speed"]
                    desc = data["weather"][0]["description"]

                    response = f"ÙÙŠ {city_ar}: Ø§Ù„Ø­Ø±Ø§Ø±Ø© {temp}Â°ØŒ {desc}ØŒ ÙˆØ§Ù„Ø±ÙŠØ§Ø­ {wind} ÙƒÙ…/Ø³ ðŸŒ¬.\n"
                    if "Ø·Ù…Ø§Ø·Ù…" in user_q:
                        response += "ðŸ… Ø§Ù„Ø·Ù…Ø§Ø·Ù… Ø¨ØªØ­Ø¨ Ø§Ù„Ø¬Ùˆ Ø§Ù„Ù…Ø¹ØªØ¯Ù„ØŒ Ù„Ùˆ Ø§Ù„Ø¬Ùˆ Ø­Ø± Ø§Ø³ØªÙ†Ù‰ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ†."
                    elif "Ù‚Ù…Ø­" in user_q:
                        response += "ðŸŒ¾ Ø§Ù„Ù‚Ù…Ø­ Ø¨ÙŠØªØ²Ø±Ø¹ Ù…Ù† Ù†Øµ Ø£ÙƒØªÙˆØ¨Ø± Ù„Ø­Ø¯ Ù†Øµ Ù†ÙˆÙÙ…Ø¨Ø±ØŒ Ø§Ø³ØªÙ†Ù‰ Ø´ÙˆÙŠØ©."
                    else:
                        if temp > 30:
                            response += "ðŸ”¥ Ø§Ù„Ø¬Ùˆ Ø­Ø±ØŒ Ù‚Ù„Ù„ Ø§Ù„Ø±ÙŠÙ‘ ÙˆØ®Ù„ÙŠ Ø¨Ø§Ù„Ùƒ Ù…Ù† Ø§Ù„Ø´ØªÙ„Ø§Øª."
                        elif temp < 15:
                            response += "â„ Ø§Ù„Ø¬Ùˆ Ø¨Ø±Ø¯ØŒ ØºØ·ÙŠ Ø§Ù„Ø²Ø±Ø¹."
                        else:
                            response += "ðŸŒ¿ Ø§Ù„Ø¬Ùˆ Ù…Ø¹ØªØ¯Ù„ØŒ ØªÙ…Ø§Ù… Ù„Ù„Ø²Ø±Ø§Ø¹Ø©."
                    st.success(response)
                else:
                    print("[WARN] Weather data missing 'main' field.")
                    st.error("Ù…Ù‚Ø¯Ø±ØªØ´ Ø£Ø¬ÙŠØ¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø·Ù‚Ø³ Ø¯Ù„ÙˆÙ‚ØªÙŠ.")
        else:
            print(f"[WARN] Place not found for query: {user_q}")
            st.error("Ù…Ø¹Ù„Ø´ØŒ Ù…Ù„Ù‚ØªØ´ Ø§Ù„Ù…ÙƒØ§Ù† Ø¯Ù‡ ÙÙŠ Ø§Ù„Ù‚Ø§ÙŠÙ…Ø©. Ø¬Ø±Ø¨ ØªÙƒØªØ¨ Ø§Ø³Ù… ØªØ§Ù†ÙŠ Ø£Ùˆ Ù‚Ø±ÙŠØ¨ Ù…Ù†Ù‡.")

# -------------------------------------------------------
# ðŸ—º Map
# -------------------------------------------------------
with tab2:
    st.subheader("Ø§Ø®ØªØ§Ø± Ù…ÙƒØ§Ù† Ù…Ù† Ù…ØµØ±:")
    city_display = st.selectbox("Ø§Ø®ØªØ± Ø§Ù„Ù…ÙƒØ§Ù†:", places_df["name"].dropna().unique())

    row = places_df[places_df["name"] == city_display].iloc[0]
    lat, lon, city_ar = row["lat"], row["lon"], row["name"]

    st.write(f"ðŸ“ Ø§Ù†Øª Ø§Ø®ØªØ±Øª: *{city_ar}*")

    m = folium.Map(location=[lat, lon], zoom_start=7)
    folium.Marker([lat, lon], tooltip=city_ar).add_to(m)
    st_folium(m, width=700, height=500)

    if OPENWEATHER_API_KEY:
        weather_url = f"http://api.openweathermap.org/data/2.5/weather?lat={lat}&lon={lon}&appid={OPENWEATHER_API_KEY}&units=metric&lang=ar"
        try:
            response = requests.get(weather_url, timeout=10).json()
            print(f"[INFO] Weather API response for {city_ar}: OK")
        except Exception as e:
            print(f"[ERROR] Weather API request failed: {e}")
            response = {}

        if "main" in response:
            temp = response["main"]["temp"]
            wind = response["wind"]["speed"]
            desc = response["weather"][0]["description"]
            st.success(f"ÙÙŠ {city_ar}: Ø§Ù„Ø­Ø±Ø§Ø±Ø© {temp}Â°ØŒ {desc}ØŒ Ø§Ù„Ø±ÙŠØ§Ø­ {wind} ÙƒÙ…/Ø³ ðŸŒ¬")
        else:
            print("[WARN] Weather data missing 'main' field.")
            st.error("Ù…Ù‚Ø¯Ø±ØªØ´ Ø£Ø¬ÙŠØ¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø·Ù‚Ø³ Ø¯Ù„ÙˆÙ‚ØªÙŠ.")

# -------------------------------------------------------
# ðŸ“° News + Prices
# -------------------------------------------------------
def extract_prices_from_articles(entries):
    price_data = []
    for item in entries:
        try:
            r = requests.get(item["link"], timeout=10)
            soup = BeautifulSoup(r.content, "lxml")
            text = soup.get_text(" ", strip=True)

            matches = re.findall(r"(\w+)\s+(\d+)\s*(?:Ø¬Ù†ÙŠÙ‡|Ø¬)", text)
            for crop, price in matches:
                price_data.append({"Ø§Ù„Ù…Ø­ØµÙˆÙ„": crop, "Ø§Ù„Ø³Ø¹Ø± (Ø¬Ù†ÙŠÙ‡)": price, "Ø§Ù„Ù…ØµØ¯Ø±": item["source"]})
            print(f"[INFO] Extracted prices from {item['link']}")
        except Exception as e:
            print(f"[ERROR] Failed to parse {item['link']}: {e}")
            continue
    return pd.DataFrame(price_data)

with tab3:
    st.subheader("ðŸ“° Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø²Ø±Ø§Ø¹ÙŠØ© + ðŸ’° Ø£Ø³Ø¹Ø§Ø± Ø§Ù„Ù…Ø­Ø§ØµÙŠÙ„ (ØªØªØ­Ø¯Ø« ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚ØªÙŠÙ†)")
    from streamlit_autorefresh import st_autorefresh
    st_autorefresh(interval=2 * 60 * 1000, key="refresh_news")

    feeds = {
        "Ø£Ø®Ø¨Ø§Ø± Ø²Ø±Ø§Ø¹ÙŠØ© (Agr-Egypt)": "https://www.agr-egypt.com/feed/",
        "Ø²Ø±Ø§Ø¹Ø© Ù…ØµØ± (Google News)": "https://news.google.com/rss/search?q=Ø²Ø±Ø§Ø¹Ø©+Ù…ØµØ±&hl=ar&gl=EG&ceid=EG:ar",
        "Ø·Ù‚Ø³ (Ø¨ÙˆØ§Ø¨Ø© Ø§Ù„Ø£Ù‡Ø±Ø§Ù…)": "https://gate.ahram.org.eg/rss/97.aspx"
    }

    entries = []
    for src_name, url in feeds.items():
        try:
            for entry in feedparser.parse(url).entries[:5]:
                entries.append({"source": src_name, "title": entry.title, "link": entry.link})
            print(f"[INFO] Loaded feed: {src_name}")
        except Exception as e:
            print(f"[ERROR] Failed to load feed {src_name}: {e}")

    if entries:
        st.markdown("### ðŸ“° Ø£Ø­Ø¯Ø« Ø§Ù„Ø£Ø®Ø¨Ø§Ø±")
        for it in entries[:10]:
            st.write(f"- *({it['source']})* [{it['title']}]({it['link']})")
    else:
        st.warning("âš  Ù…Ø´ Ù„Ø§Ù‚ÙŠØª Ø£Ø®Ø¨Ø§Ø± Ø¯Ù„ÙˆÙ‚ØªÙŠ.")

    st.markdown("### ðŸ“Š Ø¬Ø¯ÙˆÙ„ Ø£Ø³Ø¹Ø§Ø± Ø§Ù„Ù…Ø­Ø§ØµÙŠÙ„ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø© Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª")
    df_prices = extract_prices_from_articles(entries)
    if not df_prices.empty:
        st.dataframe(df_prices)
    else:
        print("[INFO] No prices extracted from articles.")
        st.info("â„¹ Ù„Ø³Ù‡ Ù…Ù„Ø§Ù‚ÙŠØªØ´ Ø£Ø³Ø¹Ø§Ø± Ù…Ø¨Ø§Ø´Ø±Ø© ÙÙŠ Ø§Ù„Ù†ØµÙˆØµØŒ Ø¨Ø³ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± ÙÙˆÙ‚ Ù…ÙˆØ¬ÙˆØ¯Ø©.")

    st.caption(f"â± Ø¢Ø®Ø± ØªØ­Ø¯ÙŠØ«: {datetime.datetime.now().strftime('%d-%m-%Y %H:%M:%S')}")
